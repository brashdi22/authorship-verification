{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyMDC5Nys6SAF4rZBTg/7Lo7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install numpy pandas collections\n","!pip install pickle\n","!pip install nltk\n","!pip install tensorflow\n","!pip install scikit-learn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8eUmG8oiNl9u","executionInfo":{"status":"ok","timestamp":1713869088568,"user_tz":-60,"elapsed":18397,"user":{"displayName":"Ismail Albrashdi","userId":"06108836221780046890"}},"outputId":"4e4a4a6c-9c1c-47de-c69f-1bca8bb3091e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement collections (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for collections\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\u001b[31m\n","\u001b[0mRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.2)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import pickle\n","import math\n","\n","import nltk\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","nltk.download('punkt')\n","\n","import tensorflow as tf\n","from tensorflow.keras import layers, Model, Input\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import load_model\n","\n","from sklearn.metrics import matthews_corrcoef, roc_auc_score, confusion_matrix, classification_report"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rPiJoxudi-q9","executionInfo":{"status":"ok","timestamp":1713869093403,"user_tz":-60,"elapsed":4848,"user":{"displayName":"Ismail Albrashdi","userId":"06108836221780046890"}},"outputId":"bde642f3-201c-4b21-e606-fb351111507d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vZmIoGIBOnen","executionInfo":{"status":"ok","timestamp":1713869113409,"user_tz":-60,"elapsed":20011,"user":{"displayName":"Ismail Albrashdi","userId":"06108836221780046890"}},"outputId":"7dc92bfb-45f0-4816-e2fd-aefdbccce30f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["##Define the paths to be used"],"metadata":{"id":"ug8ci07_1Bla"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eDNFN3b5MkjR"},"outputs":[],"source":["# All files should be in the same folder.\n","# If you are using Google drive make sure base_dir_path is the actual\n","# path to your folder, otherwise it should be base_dir_path=''.\n","\n","base_dir_path = '/content/drive/My Drive/NLU/cw/'\n","dataset_path = base_dir_path + 'test.csv'\n","siamese_model_path = base_dir_path + 'siamese_model_B.keras'\n","char_tokenizer_path = base_dir_path + 'char_tokenizer_B.pkl'\n","word_tokenizer_path = base_dir_path + 'word_tokenizer_B.pkl'\n","predictions_output_path = base_dir_path + 'Group_62_B.csv'"]},{"cell_type":"markdown","source":["The model can be found at https://livemanchesterac-my.sharepoint.com/:u:/g/personal/ismail_albrashdi_student_manchester_ac_uk/EbwaeE2pd1pMn69tY75q7OMBxfkiKGlRmrsoJSXy9urd0Q?e=V9UxbZ\n","\n","The tokenizers were uploaded to Blackboard with notebook."],"metadata":{"id":"ptPrXQHpFlwW"}},{"cell_type":"markdown","source":["##Define the helper functions needed"],"metadata":{"id":"EGBlQ3DQ9_26"}},{"cell_type":"code","source":["def load_tokenizer(path):\n","  with open(path, 'rb') as f:\n","    tokenizer = pickle.load(f)\n","  return tokenizer\n","\n","\n","def preprocess_text(df_column, char_tokenizer, word_tokenizer, max_sent_length, max_word_length, max_char_length):\n","  # Initialize empty lists to hold the padded data\n","  char_data_padded = []\n","  word_data_padded = []\n","\n","  for document in df_column:\n","    # Tokenize the document into sentences, then words, then characters\n","    sent_tokens = sent_tokenize(document)\n","    word_tokens = [word_tokenize(sent) for sent in sent_tokens]\n","    char_tokens = [[list(word) for word in sent] for sent in word_tokens]\n","\n","    # Convert tokens to sequences using the respective tokenizers\n","    char_sequences = [[[char_tokenizer.word_index.get(char, 0) for char in word] for word in sent] for sent in char_tokens]\n","    word_sequences = [[word_tokenizer.word_index.get(word, 0) for word in sent] for sent in word_tokens]\n","\n","    # Pad sequences to the same length\n","    char_sequences_padded = pad_sequences([pad_sequences(seq, maxlen=max_char_length, padding='post', truncating='post') for seq in char_sequences], maxlen=max_word_length, padding='post', truncating='post')\n","    word_sequences_padded = pad_sequences(word_sequences, maxlen=max_word_length, padding='post', truncating='post')\n","\n","    # Append the padded data to the lists\n","    char_data_padded.append(char_sequences_padded)\n","    word_data_padded.append(word_sequences_padded)\n","\n","  # Pad the lists to have uniform sentence length\n","  char_data_padded = pad_sequences(char_data_padded, maxlen=max_sent_length, padding='post', truncating='post')\n","  word_data_padded = pad_sequences(word_data_padded, maxlen=max_sent_length, padding='post', truncating='post')\n","\n","  return char_data_padded, word_data_padded\n","\n","\n","# Same custom attention layer used for training\n","class Attention(layers.Layer):\n","  \"\"\"\n","  This class implements a simple attention mechanism in a neural network layer.\n","\n","  Attributes:\n","      W (tf.Tensor): A trainable weight matrix that transforms the input features before computing\n","                     attention scores. The shape of W is (feature_dim, feature_dim) where feature_dim\n","                     is the last dimension of the input.\n","      v (tf.Tensor): A trainable vector that computes the raw attention scores from the transformed\n","                     input. It's used to convert the tanh output into a score for each feature across\n","                     the input sequence.\n","      build(input_shape): Sets up the weights of the layer based on the shape of the input it will receive.\n","      call(x): Processes the input 'x' through the attention mechanism, computes attention scores, and\n","               returns a weighted sum of the input features based on these scores.\n","\n","  Parameters:\n","      x (tf.Tensor): The input tensor to the attention layer. This is typically the output of an RNN,\n","      LSTM, or another layer that processes sequences.\n","\n","  Returns:\n","      output (tf.Tensor): A tensor where the input sequences are aggregated (via a weighted sum) based\n","                          on the learned attention scores. This output tensor typically has shape\n","                          (batch_size, features) after reducing the sequence dimension.\n","  \"\"\"\n","  def __init__(self, **kwargs):\n","    super(Attention, self).__init__(**kwargs)\n","\n","  def build(self, input_shape):\n","    self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], input_shape[-1]),\n","                              initializer='random_normal', trainable=True)\n","    self.v = self.add_weight(name='attention_score_vector', shape=(input_shape[-1], 1),\n","                              initializer='random_normal', trainable=True)\n","    super(Attention, self).build(input_shape)\n","\n","  def call(self, x):\n","    # u^(w) = tanh(W^(a) h^(w))\n","    u = tf.tanh(tf.tensordot(x, self.W, axes=[2, 0]))\n","    # Compute the raw attention scores v^(a)\n","    scores = tf.tensordot(u, self.v, axes=[2, 0])\n","    # Turn raw scores into probabilities using softmax (alpha)\n","    a = tf.nn.softmax(scores, axis=1)\n","    # Weighted sum of the input sequence\n","    output = tf.reduce_sum(x * a, axis=1)\n","    return output"],"metadata":{"id":"pghXNYeoOvqm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Define the prediction function"],"metadata":{"id":"ier6fXST94JV"}},{"cell_type":"code","source":["def predict_model_B(dataset_path, model_path, char_tokenizer_path, word_tokenizer_path, output_predictions_path):\n","  # Load the model and the tokenizers\n","  siamese_model = load_model(model_path, custom_objects={'Attention': Attention}, safe_mode=False)\n","  char_tokenizer = load_tokenizer(char_tokenizer_path)\n","  word_tokenizer = load_tokenizer(word_tokenizer_path)\n","\n","  # Define the parameters that were used for training\n","  max_sent_length = 30                                      # maximum number of sentences per document\n","  max_word_length = 50                                      # maximum number of words per sentence\n","  max_char_length = 20                                      # maximum number of characters per word\n","  batch_size = 64\n","\n","  # Load the dataset\n","  df = pd.read_csv(dataset_path)\n","  df['text_1'] = df['text_1'].astype(str)\n","  df['text_2'] = df['text_2'].astype(str)\n","\n","  # Prepare the input data\n","  char_data_1, word_data_1 = preprocess_text(df['text_1'], char_tokenizer, word_tokenizer, max_sent_length, max_word_length, max_char_length)\n","  char_data_2, word_data_2 = preprocess_text(df['text_2'], char_tokenizer, word_tokenizer, max_sent_length, max_word_length, max_char_length)\n","\n","  # Use the model to predict\n","  predictions = siamese_model.predict([np.array(char_data_1), np.array(word_data_1), np.array(char_data_2), np.array(word_data_2)])\n","\n","  # Convert the predictions to 0s and 1s\n","  threshold = 0.5\n","  binary_predictions = (predictions > threshold).astype(int)\n","\n","  # Save the predictions to a CSV file\n","  predictions_df = pd.DataFrame(binary_predictions, columns=['prediction'])\n","  predictions_df.to_csv(output_predictions_path, index=False)"],"metadata":{"id":"VuuzrvQ0kTar"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Call the prediction function with all the required paramaters"],"metadata":{"id":"fW8HR4sCrgdb"}},{"cell_type":"code","source":["predict_model_B(dataset_path, siamese_model_path, char_tokenizer_path, word_tokenizer_path, predictions_output_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"670BjpEYjief","executionInfo":{"status":"ok","timestamp":1713871108588,"user_tz":-60,"elapsed":45533,"user":{"displayName":"Ismail Albrashdi","userId":"06108836221780046890"}},"outputId":"8e36243d-6c2f-4931-e617-4d0a16852a62"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["188/188 [==============================] - 13s 53ms/step\n"]}]}]}