{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyPv4CdFzFun88hKltmiDLba"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install numpy pandas collections\n","!pip install pickle\n","!pip install nltk\n","!pip install tensorflow\n","!pip install scikit-learn\n","!pip install fasttext"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D3SVpQ9h9XkH","executionInfo":{"status":"ok","timestamp":1713864867979,"user_tz":-60,"elapsed":72648,"user":{"displayName":"Ismail Albrashdi","userId":"06108836221780046890"}},"outputId":"0d457717-a07b-44c3-c160-bac5ff15c660"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement collections (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for collections\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\u001b[31m\n","\u001b[0mRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.2)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n","Collecting fasttext\n","  Downloading fasttext-0.9.2.tar.gz (68 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pybind11>=2.2 (from fasttext)\n","  Using cached pybind11-2.12.0-py3-none-any.whl (234 kB)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.25.2)\n","Building wheels for collected packages: fasttext\n","  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4227139 sha256=ed189dee4d91ef5f84176494a1e75a52c34f8e81fd7afa7b71785d4cdb1e8275\n","  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n","Successfully built fasttext\n","Installing collected packages: pybind11, fasttext\n","Successfully installed fasttext-0.9.2 pybind11-2.12.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SCN__Lu-aYJ2","executionInfo":{"status":"ok","timestamp":1713867286275,"user_tz":-60,"elapsed":3,"user":{"displayName":"Ismail Albrashdi","userId":"06108836221780046890"}},"outputId":"276fb711-6295-43fb-c8b9-8e43e1ad9114"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import string\n","import numpy as np\n","import pandas as pd\n","import math\n","from collections import Counter\n","import pickle\n","\n","import nltk\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","\n","nltk.download('punkt')\n","\n","import tensorflow as tf\n","from tensorflow.keras import layers, Model, Input\n","from tensorflow.keras.layers import Input, Embedding, Conv1D, LSTM, Dense, Flatten, Subtract, Bidirectional, GlobalMaxPooling1D, TimeDistributed, Lambda, Concatenate, Layer, Activation, Softmax\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.models import load_model\n","\n","from sklearn.metrics import matthews_corrcoef, roc_auc_score, confusion_matrix, classification_report\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","\n","import fasttext\n"]},{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","# base_dir_path = '/content/drive/My Drive/NLU/cw/'\n","\n","base_dir_path = ''\n","training_data_path = base_dir_path + 'train.csv'\n","development_data_path = base_dir_path + 'dev.csv'"],"metadata":{"id":"DyY-OpzWOySG","executionInfo":{"status":"ok","timestamp":1713864891846,"user_tz":-60,"elapsed":18972,"user":{"displayName":"Ismail Albrashdi","userId":"06108836221780046890"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ef0533b6-ebf8-480a-e91f-f45053c249e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["##Helper functions\n"],"metadata":{"id":"Y3dRlZpgrvzy"}},{"cell_type":"code","source":["def load_fasttext_embeddings(embeddings_path, word_index, embedding_dim=300):\n","  \"\"\"\n","  Loads FastText word embeddings. This function constructs an embedding matrix\n","  that is used to initialise the weights in the embedding layer of the neural\n","  network model.\n","\n","  param embeddings_path: The path to the FastText embeddings file.\n","  param word_index: A dictionary mapping words to their indices in the embedding\n","                    matrix.\n","  param embedding_dim: The dimensionality of the word vectors.\n","\n","  return: Embedding matrix\n","  \"\"\"\n","  # Initialize the embedding matrix\n","  embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n","\n","  # Open the FastText embeddings file\n","  with open(embeddings_path, 'r', encoding='utf-8') as f:\n","      for line in f:\n","          values = line.split()\n","          word = values[0]\n","          if word in word_index:\n","              vector = np.asarray(values[1:], dtype='float32')\n","              embedding_matrix[word_index[word]] = vector\n","\n","  return embedding_matrix\n","\n","\n","def data_generator(df, char_tokenizer, word_tokenizer, batch_size, max_sent_length, max_word_length, max_char_length):\n","  \"\"\"\n","  Generator function for the data. It yields batches of data from the given dataframe to avoid running out of memory.\n","\n","  param df: the dataframe containing the data.\n","  param char_tokenizer: the character tokenizer.\n","  param word_tokenizer: the word tokenizer.\n","  param batch_size: the batch size.\n","  param max_sent_length: the maximum number of sentences allowed per document.\n","  param max_word_length: the maximum number of words allowed per sentence.\n","  param max_char_length: the maximum number of characters allowed per word.\n","\n","  yield batches of data.\n","  \"\"\"\n","  num_samples = len(df)\n","  while True:\n","    for offset in range(0, num_samples, batch_size):\n","      # Get the batch of data\n","      batch_samples = df.iloc[offset:min(offset + batch_size, num_samples)]\n","\n","      # Preprocess the text data for the current batch\n","      char_data_1, word_data_1 = preprocess_text(batch_samples['text_1'], char_tokenizer, word_tokenizer, max_sent_length, max_word_length, max_char_length)\n","      char_data_2, word_data_2 = preprocess_text(batch_samples['text_2'], char_tokenizer, word_tokenizer, max_sent_length, max_word_length, max_char_length)\n","\n","      # Get the labels\n","      batch_labels = batch_samples['label'].values\n","\n","      yield [np.array(char_data_1), np.array(word_data_1), np.array(char_data_2), np.array(word_data_2)], np.array(batch_labels)\n","\n","\n","def save_tokenizer(path, tokenizer):\n","  with open(path, 'wb') as f:\n","    pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","\n","def euclidean_distance(vectors):\n","  \"\"\"\n","  Computes the euclidean distance between 2 tensors\n","\n","  param vectors: a list containing two tensors of same length\n","\n","  returns: the euclidean distance\n","  \"\"\"\n","  vector1, vector2 = vectors\n","  sum_square = tf.reduce_sum(tf.square(vector1 - vector2), axis=1, keepdims=True)\n","  return tf.sqrt(tf.maximum(sum_square, tf.keras.backend.epsilon()))\n","\n","\n","def inverse_exponential(x):\n","  return tf.exp(-x)"],"metadata":{"id":"N3MqxeqyLmtL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Define the preprocessing function"],"metadata":{"id":"DuClMpR4LalU"}},{"cell_type":"code","source":["def preprocess_text(df_column, char_tokenizer, word_tokenizer, max_sent_length, max_word_length, max_char_length):\n","  \"\"\"\n","  Puts the text samples in the structure required by the model. The function tokenizes text at three\n","  levels: sentences, words, and characters, and applies padding to standardize lengths at each level.\n","\n","  param df_column: the column containing the text samples.\n","  param char_tokenizer: the tokenizer for character-level tokenization.\n","  param word_tokenizer: the tokenizer for word-level tokenization.\n","  param max_sent_length: maximum number of sentences allowed per document.\n","  param max_word_length: maximum number of words allowed per sentence.\n","  param max_char_length: maximum number of characters allowed per word.\n","\n","  Returns:\n","    - char_data_padded: 4D array where each entry contains the padded character-level representations of words,\n","      structured as [documents, sentences, words, chars].\n","    - word_data_padded: 3D array where each entry contains the padded word-level representations of sentences,\n","      structured as [documents, sentences, words].\n","  \"\"\"\n","  # Initialize empty lists to hold the padded data\n","  char_data_padded = []\n","  word_data_padded = []\n","\n","\n","  for document in df_column:\n","    # Tokenize the document into sentences, then words, then characters\n","    sent_tokens = sent_tokenize(document)\n","    word_tokens = [word_tokenize(sent) for sent in sent_tokens]\n","    char_tokens = [[list(word) for word in sent] for sent in word_tokens]\n","\n","    # Convert tokens to sequences using the respective tokenizers\n","    char_sequences = [[[char_tokenizer.word_index.get(char, 0) for char in word] for word in sent] for sent in char_tokens]\n","    word_sequences = [[word_tokenizer.word_index.get(word, 0) for word in sent] for sent in word_tokens]\n","\n","    # Pad sequences to the same length\n","    char_sequences_padded = pad_sequences([pad_sequences(seq, maxlen=max_char_length, padding='post', truncating='post') for seq in char_sequences], maxlen=max_word_length, padding='post', truncating='post')\n","    word_sequences_padded = pad_sequences(word_sequences, maxlen=max_word_length, padding='post', truncating='post')\n","\n","    # Append the padded data to the lists\n","    char_data_padded.append(char_sequences_padded)\n","    word_data_padded.append(word_sequences_padded)\n","\n","  # Pad the lists to have uniform sentence length\n","  char_data_padded = pad_sequences(char_data_padded, maxlen=max_sent_length, padding='post', truncating='post')\n","  word_data_padded = pad_sequences(word_data_padded, maxlen=max_sent_length, padding='post', truncating='post')\n","\n","  return char_data_padded, word_data_padded"],"metadata":{"id":"CczPdBcTB9sh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Data preparation"],"metadata":{"id":"trS7NsEnOl9C"}},{"cell_type":"markdown","source":["####Load and split the training dataset"],"metadata":{"id":"yIafDW6CsG-H"}},{"cell_type":"code","source":["df = pd.read_csv(training_data_path)\n","df['text_1'] = df['text_1'].astype(str)\n","df['text_2'] = df['text_2'].astype(str)\n","train_df, val_df = train_test_split(df, test_size=0.2, random_state=37, shuffle=True)"],"metadata":{"id":"u5J8e-8-byAy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Fit and save the tokenizers"],"metadata":{"id":"4Q6-1IsWsfeF"}},{"cell_type":"code","source":["# Tokenizers\n","char_tokenizer = Tokenizer(char_level=True)\n","word_tokenizer = Tokenizer()\n","\n","# Fit tokenizers\n","char_tokenizer.fit_on_texts(pd.concat([df['text_1'], df['text_2']], axis=0))\n","word_tokenizer.fit_on_texts(pd.concat([df['text_1'], df['text_2']], axis=0))\n","\n","# Save tokenizers\n","save_tokenizer(base_dir_path + 'char_tokenizer_B.pkl', char_tokenizer)\n","save_tokenizer(base_dir_path + 'word_tokenizer_B.pkl', word_tokenizer)"],"metadata":{"id":"i1htigmb0j9Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Load FastText word embeddings"],"metadata":{"id":"y-2MidWrPLBN"}},{"cell_type":"code","source":["word_embedding_matrix = load_fasttext_embeddings(base_dir_path + 'cc.en.300.vec', word_tokenizer.word_index)"],"metadata":{"id":"7jvoUz1i4_o_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Model Building"],"metadata":{"id":"anRH9yIYMHbo"}},{"cell_type":"markdown","source":["####Define the custom attention layer"],"metadata":{"id":"4f-VIwxuMZVy"}},{"cell_type":"code","source":["class Attention(layers.Layer):\n","  \"\"\"\n","  This class implements a simple attention mechanism in a neural network layer.\n","\n","  Attributes:\n","      W (tf.Tensor): A trainable weight matrix that transforms the input features before computing\n","                     attention scores. The shape of W is (feature_dim, feature_dim) where feature_dim\n","                     is the last dimension of the input.\n","      v (tf.Tensor): A trainable vector that computes the raw attention scores from the transformed\n","                     input. It's used to convert the tanh output into a score for each feature across\n","                     the input sequence.\n","      build(input_shape): Sets up the weights of the layer based on the shape of the input it will receive.\n","      call(x): Processes the input 'x' through the attention mechanism, computes attention scores, and\n","               returns a weighted sum of the input features based on these scores.\n","\n","  Parameters:\n","      x (tf.Tensor): The input tensor to the attention layer. This is typically the output of an RNN,\n","      LSTM, or another layer that processes sequences.\n","\n","  Returns:\n","      output (tf.Tensor): A tensor where the input sequences are aggregated (via a weighted sum) based\n","                          on the learned attention scores. This output tensor typically has shape\n","                          (batch_size, features) after reducing the sequence dimension.\n","  \"\"\"\n","  def __init__(self, **kwargs):\n","    super(Attention, self).__init__(**kwargs)\n","\n","  def build(self, input_shape):\n","    self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], input_shape[-1]),\n","                              initializer='random_normal', trainable=True)\n","    self.v = self.add_weight(name='attention_score_vector', shape=(input_shape[-1], 1),\n","                              initializer='random_normal', trainable=True)\n","    super(Attention, self).build(input_shape)\n","\n","  def call(self, x):\n","    # u^(w) = tanh(W^(a) h^(w))\n","    u = tf.tanh(tf.tensordot(x, self.W, axes=[2, 0]))\n","    # Compute the raw attention scores v^(a)\n","    scores = tf.tensordot(u, self.v, axes=[2, 0])\n","    # Turn raw scores into probabilities using softmax (alpha)\n","    a = tf.nn.softmax(scores, axis=1)\n","    # Weighted sum of the input sequence\n","    output = tf.reduce_sum(x * a, axis=1)\n","    return output"],"metadata":{"id":"-1sssJcIMr1P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Define the functions to build the model"],"metadata":{"id":"PGZtgNlSMzXB"}},{"cell_type":"code","source":["def build_document_feature_network(char_vocab_size, word_vocab_size, char_embedding_dim,\n","                                   word_embedding_dim, max_sent_length, max_word_length,\n","                                   max_char_length, lstm_units, mlp_units,\n","                                   word_embedding_matrix, h):\n","  \"\"\"\n","  Builds a branch of the siamese model. Each branch works independently on to extract the document features\n","  from the input documents (text_1 and text_2).\n","\n","  The network is based on the ADHOMINEM architecture proposed by Boenninghoff et al 2019:\n","    - Boenninghoff, B., Hessler, S., Kolossa, D. and Nickel, R.M., 2019, December. Explainable\n","      authorship verification in social media via attention-based similarity learning. In 2019\n","      IEEE International Conference on Big Data (Big Data) (pp. 36-45). IEEE.\n","\n","  In summary:\n","  - the model uses a CNN + global-max-pooling to exatract information from char embeddeings (to gain insight\n","    into affixes and suffixes).\n","  - the output of global-max-pooling is concatenated with the corresponding word embedding to get the word\n","    representation of the document.\n","  - the word representation of the document is fed into a BiLSTM then Attention to get the sentence representation\n","    of the document.\n","  - the sentence representation of the document is fed into a BiLSTM then Attention to get the document representation.\n","  - the document representation is fed into a MLP to get the document features.\n","\n","  TimeDistributed is used to process the inputs in hierarchical order.\n","\n","  param char_vocab_size: the number of unique characters in the character vocabulary.\n","  param word_vocab_size: the number of unique words in the word vocabulary.\n","  param char_embedding_dim: the dimensionality of the character embeddings.\n","  param word_embedding_dim: the dimensionality of the word embeddings.\n","  param max_sent_length: the maximum number of sentences allowed per document.\n","  param max_word_length: the maximum number of words allowed per sentence.\n","  param max_char_length: the maximum number of characters allowed per word.\n","  param lstm_units: the number of units in the LSTM layers.\n","  param mlp_units: the number of units in the MLP layer.\n","  param word_embedding_matrix: the word embedding matrix.\n","  param h: the size of the kernel in the CNN.\n","\n","  return: the document feature network.\n","\n","  \"\"\"\n","  # Input Layers\n","  char_input = Input(shape=(max_sent_length, max_word_length, max_char_length), dtype='int32', name='char_input')\n","  word_input = Input(shape=(max_sent_length, max_word_length), dtype='int32', name='word_input')\n","\n","  # Character Embedding and Convolution\n","  char_embeddings = TimeDistributed(TimeDistributed(Embedding(input_dim=char_vocab_size, output_dim=char_embedding_dim)))(char_input)\n","\n","  conv1d_out = TimeDistributed(TimeDistributed(Conv1D(filters=char_embedding_dim, kernel_size=h, activation='tanh')))(char_embeddings)\n","  char_representations = TimeDistributed(TimeDistributed(GlobalMaxPooling1D()))(conv1d_out)\n","\n","  # Word Embeddings\n","  embedding_layer = Embedding(input_dim=word_vocab_size,\n","                            output_dim=300,\n","                            weights=[word_embedding_matrix],\n","                            trainable=False)\n","  word_embeddings = TimeDistributed(embedding_layer)(word_input)\n","\n","  # Combine Char and Word Representations\n","  combined_representations = TimeDistributed(Concatenate())([char_representations, word_embeddings])\n","\n","  # Word to Sentence Encoding with BiLSTM\n","  sentence_encoder = TimeDistributed(Bidirectional(LSTM(lstm_units, return_sequences=True)))(combined_representations)\n","  sentence_attention = TimeDistributed(Attention())(sentence_encoder)\n","\n","  # Sentence to Document Encoding with BiLSTM\n","  document_encoder = Bidirectional(LSTM(lstm_units, return_sequences=True))(sentence_attention)\n","  document_representation = Attention()(document_encoder)\n","\n","  # Document_representation to document features\n","  document_features = Dense(mlp_units, activation='tanh')(document_representation)\n","\n","  model = Model(inputs=[char_input, word_input], outputs=document_features, name='document_feature_network')\n","  return model\n","\n","def build_siamese_model(document_feature_network, max_sent_length, max_word_length, max_char_length):\n","  \"\"\"\n","  Builds the siamese model. It defines 4 inputs (2 for each input document), creates 2 branches of the\n","  document feature network to extract the document features from the input documents, and computes the\n","  similarity score between the two document features.\n","\n","\n","  param document_feature_network: the document feature network.\n","  param max_sent_length: the maximum number of sentences allowed per document.\n","  param max_word_length: the maximum number of words allowed per sentence.\n","  param max_char_length: the maximum number of characters allowed per word.\n","\n","  return: the siamese model.\n","  \"\"\"\n","  # Inputs for two documents\n","  char_input_1 = Input(shape=(max_sent_length, max_word_length, max_char_length), dtype='int32', name='char_input_1')\n","  word_input_1 = Input(shape=(max_sent_length, max_word_length), dtype='int32', name='word_input_1')\n","\n","  char_input_2 = Input(shape=(max_sent_length, max_word_length, max_char_length), dtype='int32', name='char_input_2')\n","  word_input_2 = Input(shape=(max_sent_length, max_word_length), dtype='int32', name='word_input_2')\n","\n","  # Generate document features for both documents\n","  document_features_1 = document_feature_network([char_input_1, word_input_1])\n","  document_features_2 = document_feature_network([char_input_2, word_input_2])\n","\n","  # Compute a similarity score between the two document feature vectors\n","  distance = Lambda(euclidean_distance)([document_features_1, document_features_2])\n","  similarity_score = Lambda(inverse_exponential)(distance)\n","\n","  # Construct the Siamese model\n","  siamese_model = Model(inputs=[char_input_1, word_input_1, char_input_2, word_input_2], outputs=similarity_score, name='siamese_document_network')\n","\n","  return siamese_model"],"metadata":{"id":"Wa-fI5djM-0g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Build the model"],"metadata":{"id":"wrVj47acNQ8X"}},{"cell_type":"code","source":["# Parameters\n","char_vocab_size = len(char_tokenizer.word_index) + 1      # number of unique chars in the chars tokenizer\n","word_vocab_size = len(word_tokenizer.word_index) + 1      # number of unique words in the words tokenizer\n","char_embedding_dim = 100                                  # character embedding dimension\n","word_embedding_dim = 300                                  # word embedding dimension\n","lstm_units = 64                                           # number of units in the LSTM layer\n","mlp_units = 128                                           # number of units in the MLP layer\n","h = 5                                                     # kernel size for the CNN\n","max_sent_length = 30                                      # maximum number of sentences per document\n","max_word_length = 50                                      # maximum number of words per sentence\n","max_char_length = 20                                      # maximum number of characters per word\n","batch_size = 64                                           # batch size for training\n","epochs = 20                                               # number of epochs for training\n","\n","\n","# Build the document feature network\n","document_feature_network = build_document_feature_network(char_vocab_size, word_vocab_size, char_embedding_dim,\n","                                                          word_embedding_dim, max_sent_length, max_word_length,\n","                                                          max_char_length, lstm_units, mlp_units,\n","                                                          word_embedding_matrix, h)\n","\n","# Build the Siamese model\n","siamese_model = build_siamese_model(document_feature_network, max_sent_length, max_word_length, max_char_length)\n","\n","# Compile the model\n","siamese_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","siamese_model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P85lcv9Zghm3","executionInfo":{"status":"ok","timestamp":1713864919427,"user_tz":-60,"elapsed":4392,"user":{"displayName":"Ismail Albrashdi","userId":"06108836221780046890"}},"outputId":"643a0644-b953-46e7-c87b-077e154747c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"siamese_document_network\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," char_input_1 (InputLayer)   [(None, 30, 50, 20)]         0         []                            \n","                                                                                                  \n"," word_input_1 (InputLayer)   [(None, 30, 50)]             0         []                            \n","                                                                                                  \n"," char_input_2 (InputLayer)   [(None, 30, 50, 20)]         0         []                            \n","                                                                                                  \n"," word_input_2 (InputLayer)   [(None, 30, 50)]             0         []                            \n","                                                                                                  \n"," document_feature_network (  (None, 128)                  3233453   ['char_input_1[0][0]',        \n"," Functional)                                              2          'word_input_1[0][0]',        \n","                                                                     'char_input_2[0][0]',        \n","                                                                     'word_input_2[0][0]']        \n","                                                                                                  \n"," lambda (Lambda)             (None, 1)                    0         ['document_feature_network[0][\n","                                                                    0]',                          \n","                                                                     'document_feature_network[1][\n","                                                                    0]']                          \n","                                                                                                  \n"," lambda_1 (Lambda)           (None, 1)                    0         ['lambda[0][0]']              \n","                                                                                                  \n","==================================================================================================\n","Total params: 32334532 (123.35 MB)\n","Trainable params: 448432 (1.71 MB)\n","Non-trainable params: 31886100 (121.64 MB)\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["## Model training"],"metadata":{"id":"ByOkTT3FNUue"}},{"cell_type":"code","source":["# Create the training and validation generators\n","train_generator = data_generator(train_df, char_tokenizer, word_tokenizer, batch_size, max_sent_length, max_word_length, max_char_length)\n","val_generator = data_generator(val_df, char_tokenizer, word_tokenizer, batch_size, max_sent_length, max_word_length, max_char_length)\n","\n","\n","# Calculate the steps per epoch for training and validation\n","steps_per_epoch = math.ceil(len(train_df) / batch_size)\n","validation_steps = math.ceil(len(val_df) / batch_size)\n","\n","# Define the callbacks\n","checkpoint = ModelCheckpoint(base_dir_path + 'best_model_B.keras', save_best_only=True, monitor='val_loss', mode='min')\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=1, mode='min', min_lr=0.001)\n","\n","# Train the model\n","history = siamese_model.fit(\n","    x=train_generator,\n","    steps_per_epoch=steps_per_epoch,\n","    epochs=epochs,\n","    validation_data=val_generator,\n","    validation_steps=validation_steps,\n","    callbacks=[checkpoint, early_stopping, reduce_lr]\n",")\n","\n","# Save the model\n","siamese_model.save(base_dir_path + 'siamese_model_B.keras')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tst5n-xB3nOo","executionInfo":{"status":"ok","timestamp":1713867285465,"user_tz":-60,"elapsed":2342455,"user":{"displayName":"Ismail Albrashdi","userId":"06108836221780046890"}},"outputId":"43b968b2-aa60-4b28-ee68-382670e427fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","375/375 [==============================] - 211s 491ms/step - loss: 0.7208 - accuracy: 0.5628 - val_loss: 0.6787 - val_accuracy: 0.5733 - lr: 0.0010\n","Epoch 2/20\n","375/375 [==============================] - 163s 435ms/step - loss: 0.6609 - accuracy: 0.6008 - val_loss: 0.6672 - val_accuracy: 0.5855 - lr: 0.0010\n","Epoch 3/20\n","375/375 [==============================] - 164s 438ms/step - loss: 0.6447 - accuracy: 0.6144 - val_loss: 0.6447 - val_accuracy: 0.6142 - lr: 0.0010\n","Epoch 4/20\n","375/375 [==============================] - 164s 438ms/step - loss: 0.6187 - accuracy: 0.6469 - val_loss: 0.6145 - val_accuracy: 0.6390 - lr: 0.0010\n","Epoch 5/20\n","375/375 [==============================] - 164s 438ms/step - loss: 0.5926 - accuracy: 0.6683 - val_loss: 0.5963 - val_accuracy: 0.6522 - lr: 0.0010\n","Epoch 6/20\n","375/375 [==============================] - 164s 437ms/step - loss: 0.5690 - accuracy: 0.6903 - val_loss: 0.5758 - val_accuracy: 0.6738 - lr: 0.0010\n","Epoch 7/20\n","375/375 [==============================] - 165s 440ms/step - loss: 0.5451 - accuracy: 0.7104 - val_loss: 0.5663 - val_accuracy: 0.6800 - lr: 0.0010\n","Epoch 8/20\n","375/375 [==============================] - 164s 438ms/step - loss: 0.5261 - accuracy: 0.7254 - val_loss: 0.5626 - val_accuracy: 0.6788 - lr: 0.0010\n","Epoch 9/20\n","375/375 [==============================] - 164s 438ms/step - loss: 0.5106 - accuracy: 0.7370 - val_loss: 0.5587 - val_accuracy: 0.6818 - lr: 0.0010\n","Epoch 10/20\n","375/375 [==============================] - 164s 436ms/step - loss: 0.5015 - accuracy: 0.7460 - val_loss: 0.5597 - val_accuracy: 0.6830 - lr: 0.0010\n","Epoch 11/20\n","375/375 [==============================] - 163s 436ms/step - loss: 0.4923 - accuracy: 0.7551 - val_loss: 0.5606 - val_accuracy: 0.6833 - lr: 0.0010\n","Epoch 12/20\n","375/375 [==============================] - 163s 435ms/step - loss: 0.4836 - accuracy: 0.7638 - val_loss: 0.5646 - val_accuracy: 0.6803 - lr: 0.0010\n","Epoch 13/20\n","375/375 [==============================] - 163s 436ms/step - loss: 0.4759 - accuracy: 0.7711 - val_loss: 0.5642 - val_accuracy: 0.6787 - lr: 0.0010\n","Epoch 14/20\n","375/375 [==============================] - 164s 436ms/step - loss: 0.4648 - accuracy: 0.7808 - val_loss: 0.5645 - val_accuracy: 0.6860 - lr: 0.0010\n","Epoch 14: early stopping\n"]}]},{"cell_type":"markdown","source":["##Model Evaluation"],"metadata":{"id":"J7ftBKY9NalV"}},{"cell_type":"markdown","source":["####Load the development dataset and set up the data generator"],"metadata":{"id":"MaQVyDPh1K9R"}},{"cell_type":"code","source":["# Load the development dataset\n","development_df = pd.read_csv(development_data_path)\n","development_df['text_1'] = development_df['text_1'].astype(str)\n","development_df['text_2'] = development_df['text_2'].astype(str)\n","dev_labels = development_df['label'].values\n","\n","# Setup the data generator\n","development_generator = data_generator(development_df, char_tokenizer, word_tokenizer, batch_size, max_sent_length, max_word_length, max_char_length)"],"metadata":{"id":"69x2wuzGLAhb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Use the model to classify the pairs of text from the development dataset"],"metadata":{"id":"2-4tIA3l1ba7"}},{"cell_type":"code","source":["# Predict using the model\n","predictions = siamese_model.predict(development_generator, steps=math.ceil(len(development_df) / batch_size))\n","\n","# Set a threshold and use it to map the predictions to 0s and 1s.\n","threshold = 0.5\n","binary_predictions = (predictions > threshold).astype(int)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1bZZK2WPnob9","executionInfo":{"status":"ok","timestamp":1713870774593,"user_tz":-60,"elapsed":18656,"user":{"displayName":"Ismail Albrashdi","userId":"06108836221780046890"}},"outputId":"076779cb-a09e-473b-ecad-628cc3533bfc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["94/94 [==============================] - 18s 192ms/step\n"]}]},{"cell_type":"markdown","source":["####Print the evaluation metrics"],"metadata":{"id":"haOaKwiL12f1"}},{"cell_type":"code","source":["# Calculate Matthews Correlation Coefficient\n","mcc = matthews_corrcoef(dev_labels, binary_predictions)\n","\n","# Calculate ROC-AUC Score\n","roc_auc = roc_auc_score(dev_labels, binary_predictions)\n","\n","# Calculate confusion matrix to get specificity and false positive rate\n","tn, fp, fn, tp = confusion_matrix(dev_labels, binary_predictions).ravel()\n","\n","# Specificity = TN / (TN + FP)\n","specificity = tn / (tn + fp)\n","\n","# False Positive Rate = FP / (FP + TN)\n","false_positive_rate = fp / (fp + tn)\n","\n","# Generate classification report\n","class_report = classification_report(dev_labels, binary_predictions, target_names=['Different Authors', 'Same Authors'])\n","\n","print(\"Matthew's Correlation Coefficient:\", mcc)\n","print(\"ROC-AUC Score:\", roc_auc)\n","print(\"Specificity:\", specificity)\n","print(\"False Positive Rate:\", false_positive_rate)\n","print(\"Classification Report:\")\n","print(class_report)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2T36Q7dMyVUt","executionInfo":{"status":"ok","timestamp":1713870783225,"user_tz":-60,"elapsed":218,"user":{"displayName":"Ismail Albrashdi","userId":"06108836221780046890"}},"outputId":"3ca9d8f9-b040-4539-d8dd-9a89fe2f9412"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Matthew's Correlation Coefficient: 0.34375964015159816\n","ROC-AUC Score: 0.6714633607851838\n","Specificity: 0.7069253931080629\n","False Positive Rate: 0.2930746068919371\n","Classification Report:\n","                   precision    recall  f1-score   support\n","\n","Different Authors       0.66      0.71      0.68      2989\n","     Same Authors       0.69      0.64      0.66      3011\n","\n","         accuracy                           0.67      6000\n","        macro avg       0.67      0.67      0.67      6000\n","     weighted avg       0.67      0.67      0.67      6000\n","\n"]}]}]}